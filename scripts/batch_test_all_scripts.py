#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ‰¹é‡æµ‹è¯•æ‰€æœ‰å‰§æœ¬æ–‡ä»¶çš„è¯„ä¼°è„šæœ¬

è¯¥è„šæœ¬ä¼šå¯¹ script_examples/scripts/ ç›®å½•ä¸‹çš„æ‰€æœ‰å‰§æœ¬æ–‡ä»¶è¿›è¡Œå…¨æµç¨‹è¯„ä¼°æµ‹è¯•ï¼Œ
å¹¶ç”Ÿæˆè¯¦ç»†çš„æµ‹è¯•æŠ¥å‘Šã€‚
"""

import os
import sys
import json
import re
import time
from datetime import datetime
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from src.utils.file_handler import FileHandler
from src.utils.logger import get_logger, OperationLogger
from src.utils.performance import PerformanceProfiler
from src.evaluators.main_evaluator import ScriptEvaluator, EvaluationConfig
from src.llm.deepseek_client import DeepSeekClient, DeepSeekConfig
from src.models.scene_models import validate_script_json

logger = get_logger(__name__)

# æ–‡ä»¶å¤§å°é˜ˆå€¼ï¼šè¶…è¿‡æ­¤å¤§å°ä½¿ç”¨ reasoner æ¨¡å‹ï¼ˆå­—ç¬¦æ•°ï¼‰
LARGE_FILE_THRESHOLD = 12000  # çº¦12Kå­—ç¬¦


def load_prompt_template(template_name: str) -> str:
    """åŠ è½½promptæ¨¡æ¿"""
    prompt_path = project_root / "prompts" / template_name
    with open(prompt_path, "r", encoding="utf-8") as f:
        return f.read()


def convert_script_to_json_internal(
    llm_client_chat: DeepSeekClient,
    llm_client_reasoner: DeepSeekClient,
    script_text: str,
    scene_type: str = "standard"
) -> list:
    """
    å†…éƒ¨è½¬æ¢å‡½æ•°ï¼Œå°†å‰§æœ¬æ–‡æœ¬è½¬æ¢ä¸ºJSON

    æ™ºèƒ½é€‰æ‹©æ¨¡å‹ï¼š
    - å°æ–‡ä»¶(<12Kå­—ç¬¦): ä½¿ç”¨ deepseek-chat (8Kè¾“å‡ºé™åˆ¶)
    - å¤§æ–‡ä»¶(>=12Kå­—ç¬¦): ä½¿ç”¨ deepseek-reasoner (64Kè¾“å‡ºé™åˆ¶)

    Args:
        llm_client_chat: DeepSeek Chatå®¢æˆ·ç«¯
        llm_client_reasoner: DeepSeek Reasonerå®¢æˆ·ç«¯
        script_text: å‰§æœ¬æ–‡æœ¬
        scene_type: åœºæ™¯ç±»å‹

    Returns:
        è½¬æ¢åçš„JSONæ•°æ®ï¼ˆåœºæ™¯åˆ—è¡¨ï¼‰
    """
    # åŠ è½½promptæ¨¡æ¿
    template_name = (
        "scene1_extraction.txt" if scene_type == "standard" else "scene2_extraction.txt"
    )
    prompt_template = load_prompt_template(template_name)

    # å¡«å……prompt
    prompt = prompt_template.replace("{script_text}", script_text)

    # æ ¹æ®æ–‡ä»¶å¤§å°æ™ºèƒ½é€‰æ‹©æ¨¡å‹
    file_size = len(script_text)
    is_large_file = file_size >= LARGE_FILE_THRESHOLD

    if is_large_file:
        model_info = f"å¤§æ–‡ä»¶ ({file_size} å­—ç¬¦) - ä½¿ç”¨ deepseek-reasoner æ¨¡å‹ (32Kè¾“å‡º)"
        logger.info(model_info)
        print(f"  âš¡ {model_info}")
        llm_client = llm_client_reasoner
        max_tokens = 32768  # reasoneræ¨¡å‹é»˜è®¤32Kè¾“å‡º
    else:
        model_info = f"æ™®é€šæ–‡ä»¶ ({file_size} å­—ç¬¦) - ä½¿ç”¨ deepseek-chat æ¨¡å‹ (8Kè¾“å‡º)"
        logger.info(model_info)
        print(f"  ğŸ’¬ {model_info}")
        llm_client = llm_client_chat
        max_tokens = 8192  # chatæ¨¡å‹æœ€å¤§8Kè¾“å‡º

    # è°ƒç”¨LLM
    response = llm_client.complete(prompt=prompt, temperature=0.1, max_tokens=max_tokens)

    # æå–å¹¶æ¸…ç†JSON
    response_text = response["content"].strip()
    json_text = extract_and_clean_json(response_text, is_large_file)

    # è§£æJSON
    try:
        json_data = json.loads(json_text)
        return json_data
    except json.JSONDecodeError as e:
        logger.error(f"JSONè§£æå¤±è´¥: {e}")
        logger.debug(f"é—®é¢˜JSONç‰‡æ®µ: {json_text[:500]}...")

        # å°è¯•ä¿®å¤å¸¸è§çš„JSONé—®é¢˜
        fixed_json = attempt_json_repair(json_text)
        if fixed_json:
            logger.info("JSONä¿®å¤æˆåŠŸï¼Œé‡æ–°è§£æ")
            return json.loads(fixed_json)
        else:
            raise


def extract_and_clean_json(response_text: str, is_large_file: bool = False) -> str:
    """
    ä»LLMå“åº”ä¸­æå–å¹¶æ¸…ç†JSON

    å¤„ç†å¤šç§æƒ…å†µï¼š
    1. markdownä»£ç å—åŒ…è£¹
    2. reasoneræ¨¡å‹çš„æ€è€ƒè¿‡ç¨‹
    3. é¢å¤–çš„è§£é‡Šæ–‡å­—

    Args:
        response_text: LLMåŸå§‹å“åº”
        is_large_file: æ˜¯å¦ä¸ºå¤§æ–‡ä»¶ï¼ˆä½¿ç”¨reasoneræ¨¡å‹ï¼‰

    Returns:
        æ¸…ç†åçš„JSONå­—ç¬¦ä¸²
    """
    text = response_text.strip()

    # 1. ç§»é™¤reasoneræ¨¡å‹å¯èƒ½çš„æ€è€ƒæ ‡è®°
    # DeepSeek reasonerå¯èƒ½ä½¿ç”¨<think>...</think>æˆ–ç±»ä¼¼æ ‡è®°
    if is_large_file:
        # ç§»é™¤<think>æ ‡ç­¾åŠå…¶å†…å®¹
        text = re.sub(r'<think>.*?</think>', '', text, flags=re.DOTALL)
        text = re.sub(r'<thinking>.*?</thinking>', '', text, flags=re.DOTALL)
        # ç§»é™¤å¯èƒ½çš„"è®©æˆ‘æ€è€ƒ"ç­‰æ–‡å­—
        text = re.sub(r'^.*?(è®©æˆ‘|é¦–å…ˆ|åˆ†æ|æ€è€ƒ).*?[\n\r]', '', text, flags=re.MULTILINE)

    # 2. æå–markdownä»£ç å—ä¸­çš„JSON
    if "```json" in text:
        start = text.find("```json") + 7
        end = text.find("```", start)
        if end > start:
            text = text[start:end].strip()
    elif "```" in text:
        start = text.find("```") + 3
        end = text.find("```", start)
        if end > start:
            text = text[start:end].strip()

    # 3. æŸ¥æ‰¾JSONæ•°ç»„çš„å¼€å§‹å’Œç»“æŸ
    # å¯»æ‰¾ç¬¬ä¸€ä¸ª [ å’Œæœ€åä¸€ä¸ª ]
    first_bracket = text.find('[')
    last_bracket = text.rfind(']')

    if first_bracket != -1 and last_bracket != -1 and last_bracket > first_bracket:
        text = text[first_bracket:last_bracket + 1]

    # 4. æ¸…ç†å¸¸è§é—®é¢˜
    # ç§»é™¤BOMæ ‡è®°
    text = text.lstrip('\ufeff')
    # ç§»é™¤å‰åçš„éJSONå­—ç¬¦
    text = text.strip()

    return text


def attempt_json_repair(json_text: str) -> str:
    """
    å°è¯•ä¿®å¤å¸¸è§çš„JSONé—®é¢˜

    Args:
        json_text: å¯èƒ½æŸåçš„JSONå­—ç¬¦ä¸²

    Returns:
        ä¿®å¤åçš„JSONå­—ç¬¦ä¸²ï¼Œå¦‚æœæ— æ³•ä¿®å¤åˆ™è¿”å›None
    """
    try:
        # 1. ä¿®å¤æœªè½¬ä¹‰çš„å¼•å·
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ç‰ˆæœ¬ï¼Œå®é™…æƒ…å†µå¯èƒ½æ›´å¤æ‚

        # 2. ä¿®å¤å°¾éƒ¨ç¼ºå°‘çš„æ‹¬å·
        if json_text.count('[') > json_text.count(']'):
            # è¡¥å……ç¼ºå¤±çš„ ]
            missing_brackets = json_text.count('[') - json_text.count(']')
            json_text += ']' * missing_brackets
            logger.info(f"è¡¥å……äº† {missing_brackets} ä¸ªç¼ºå¤±çš„ ]")

        # 3. ç§»é™¤å°¾éƒ¨çš„é€—å·
        json_text = re.sub(r',\s*([\]}])', r'\1', json_text)

        # 4. å°è¯•è§£æï¼Œå¦‚æœæˆåŠŸå°±è¿”å›
        json.loads(json_text)
        return json_text

    except Exception as e:
        logger.error(f"JSONä¿®å¤å¤±è´¥: {e}")
        return None


def run_batch_test(use_llm_judge=True):
    """
    æ‰¹é‡æµ‹è¯•æ‰€æœ‰å‰§æœ¬æ–‡ä»¶

    Args:
        use_llm_judge: æ˜¯å¦ä½¿ç”¨LLMè¯­ä¹‰è¯„ä¼°ï¼ˆé»˜è®¤Trueï¼Œå®Œæ•´æµ‹è¯•ï¼‰
    """
    print("=" * 70)
    print("å‰§æœ¬æ‰¹é‡æµ‹è¯•ç³»ç»Ÿ")
    print("=" * 70)
    print()

    profiler = PerformanceProfiler("æ‰¹é‡æµ‹è¯•")
    profiler.start("åˆå§‹åŒ–")

    # åˆå§‹åŒ–ç»„ä»¶
    file_handler = FileHandler()
    config = EvaluationConfig(
        use_deepseek_judge=use_llm_judge,
        save_detailed_report=True
    )
    evaluator = ScriptEvaluator(config)

    # åˆå§‹åŒ–ä¸¤ä¸ªLLMå®¢æˆ·ç«¯ï¼šchatæ¨¡å‹å’Œreasoneræ¨¡å‹
    import os
    api_key = os.getenv("DEEPSEEK_API_KEY", "")

    chat_config = DeepSeekConfig(api_key=api_key, model="deepseek-chat")
    llm_client_chat = DeepSeekClient(config=chat_config, debug=True)

    reasoner_config = DeepSeekConfig(api_key=api_key, model="deepseek-reasoner")
    llm_client_reasoner = DeepSeekClient(config=reasoner_config, debug=True)

    logger.info(f"åˆå§‹åŒ–å®Œæˆ - Chatæ¨¡å‹: {chat_config.model} (max_tokens={chat_config.max_tokens})")
    logger.info(f"åˆå§‹åŒ–å®Œæˆ - Reasoneræ¨¡å‹: {reasoner_config.model} (max_tokens={reasoner_config.max_tokens})")

    # è·å–æ‰€æœ‰å‰§æœ¬æ–‡ä»¶
    scripts_dir = project_root / "script_examples" / "scripts"
    script_files = sorted([f for f in scripts_dir.glob("*.md")])

    print(f"æ‰¾åˆ° {len(script_files)} ä¸ªå‰§æœ¬æ–‡ä»¶")
    print(f"ä½¿ç”¨LLMè¯­ä¹‰è¯„ä¼°: {'æ˜¯' if use_llm_judge else 'å¦'}")
    print()

    profiler.checkpoint("å¼€å§‹æµ‹è¯•")

    # æµ‹è¯•ç»“æœ
    results = []
    success_count = 0
    fail_count = 0

    # é€ä¸ªæµ‹è¯•
    for idx, script_file in enumerate(script_files, 1):
        print(f"\n[{idx}/{len(script_files)}] æµ‹è¯•æ–‡ä»¶: {script_file.name}")
        print("-" * 70)

        test_start_time = time.time()

        try:
            with OperationLogger(logger, f"æµ‹è¯• {script_file.name}"):
                # è¯»å–å‰§æœ¬
                script_text = file_handler.read_text_file(str(script_file))
                print(f"  æ–‡ä»¶å¤§å°: {len(script_text)} å­—ç¬¦")

                # è½¬æ¢ä¸ºJSON
                print(f"  æ­£åœ¨è°ƒç”¨DeepSeek APIè½¬æ¢å‰§æœ¬...")
                json_data = convert_script_to_json_internal(
                    llm_client_chat=llm_client_chat,
                    llm_client_reasoner=llm_client_reasoner,
                    script_text=script_text,
                    scene_type="standard"
                )

                if not json_data:
                    raise ValueError("è½¬æ¢å¤±è´¥ï¼šæœªèƒ½æå–åœºæ™¯æ•°æ®")

                scene_count = len(json_data)
                print(f"  âœ… è½¬æ¢æˆåŠŸï¼ŒåŒ…å« {scene_count} ä¸ªåœºæ™¯")

                # è¿è¡Œè¯„ä¼°
                print(f"  æ­£åœ¨è¿è¡Œè´¨é‡è¯„ä¼°...")
                result = evaluator.evaluate_script(
                    source_text=script_text,
                    extracted_json=json_data,
                    scene_type="standard",
                    source_file=script_file.name
                )

                test_duration = time.time() - test_start_time

                # è®°å½•ç»“æœ
                test_result = {
                    "file": script_file.name,
                    "status": "æˆåŠŸ",
                    "duration": round(test_duration, 2),
                    "scene_count": scene_count,
                    "overall_score": round(result.overall_score, 3),
                    "quality_level": result.quality_level,
                    "passed": "æ˜¯" if result.passed else "å¦",
                    "scores": {
                        "structure": round(result.structure_score, 3),
                        "boundary": round(result.boundary_score, 3),
                        "character": round(result.character_score, 3),
                        "semantic": round(result.semantic_score, 3) if use_llm_judge else 0.0
                    }
                }

                results.append(test_result)
                success_count += 1

                # æ˜¾ç¤ºç»“æœ
                print(f"\n  è¯„ä¼°ç»“æœ:")
                print(f"    è´¨é‡çº§åˆ«: {result.quality_level}")
                print(f"    æ€»åˆ†: {result.overall_score:.3f}")
                print(f"    é€šè¿‡: {'âœ… æ˜¯' if result.passed else 'âŒ å¦'}")
                print(f"    è€—æ—¶: {test_duration:.2f}ç§’")

        except Exception as e:
            test_duration = time.time() - test_start_time
            logger.error(f"æµ‹è¯•å¤±è´¥: {script_file.name}", exc_info=True)

            test_result = {
                "file": script_file.name,
                "status": "å¤±è´¥",
                "duration": round(test_duration, 2),
                "error": str(e)
            }

            results.append(test_result)
            fail_count += 1

            print(f"  âŒ æµ‹è¯•å¤±è´¥: {str(e)}")

    profiler.checkpoint("æµ‹è¯•å®Œæˆ")

    # ç”ŸæˆæŠ¥å‘Š
    print("\n" + "=" * 70)
    print("æµ‹è¯•æ±‡æ€»")
    print("=" * 70)
    print(f"æ€»æ–‡ä»¶æ•°: {len(script_files)}")
    print(f"æˆåŠŸ: {success_count}")
    print(f"å¤±è´¥: {fail_count}")
    print(f"æˆåŠŸç‡: {(success_count/len(script_files)*100):.1f}%")
    print()

    # ç»Ÿè®¡ä¿¡æ¯
    if success_count > 0:
        successful_results = [r for r in results if r["status"] == "æˆåŠŸ"]
        avg_score = sum(r["overall_score"] for r in successful_results) / len(successful_results)
        avg_duration = sum(r["duration"] for r in successful_results) / len(successful_results)

        print("æˆåŠŸæµ‹è¯•ç»Ÿè®¡:")
        print(f"  å¹³å‡åˆ†æ•°: {avg_score:.3f}")
        print(f"  å¹³å‡è€—æ—¶: {avg_duration:.2f}ç§’")

        # è´¨é‡çº§åˆ«åˆ†å¸ƒ
        quality_dist = {}
        for r in successful_results:
            level = r["quality_level"]
            quality_dist[level] = quality_dist.get(level, 0) + 1

        print(f"\n  è´¨é‡çº§åˆ«åˆ†å¸ƒ:")
        for level, count in sorted(quality_dist.items()):
            print(f"    {level}: {count} ä¸ª")

    # å¤±è´¥è¯¦æƒ…
    if fail_count > 0:
        print(f"\nå¤±è´¥è¯¦æƒ…:")
        failed_results = [r for r in results if r["status"] == "å¤±è´¥"]
        for r in failed_results:
            print(f"  - {r['file']}: {r.get('error', 'æœªçŸ¥é”™è¯¯')}")

    profiler.stop()
    print()
    profiler.print_report()

    # ä¿å­˜è¯¦ç»†æŠ¥å‘Š
    report_file = project_root / "outputs" / "reports" / f"batch_test_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    report_file.parent.mkdir(parents=True, exist_ok=True)

    report_data = {
        "test_time": datetime.now().isoformat(),
        "use_llm_judge": use_llm_judge,
        "total_files": len(script_files),
        "success_count": success_count,
        "fail_count": fail_count,
        "success_rate": round(success_count/len(script_files)*100, 2),
        "results": results
    }

    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report_data, f, ensure_ascii=False, indent=2)

    print(f"\nè¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
    print()
    print("=" * 70)
    print("æ‰¹é‡æµ‹è¯•å®Œæˆ!")
    print("=" * 70)

    return results


if __name__ == "__main__":
    # æ£€æŸ¥å‘½ä»¤è¡Œå‚æ•°
    use_llm = True
    if len(sys.argv) > 1 and sys.argv[1] == "--no-llm-judge":
        use_llm = False

    run_batch_test(use_llm_judge=use_llm)
